RAM Modeling

Addiiton, mult, comparison = 1 step
Memory read/write= 1step


Big O
measures number of steps grows
as the input size n grows
it measure growth rate, not runtime
what is the shape of the curve?

O(n²) = “you’ll never go faster than n² steps”

Ω(n) = “you’ll always go at least n steps”

Θ(n log n) = “you consistently hover around n log n steps”


===========

f(n) = Your actual travel time in a specific car

g(n) = Travel time of a rocket bike

If you say "My car is O(rocket bike)", you're saying: 
On a long enough road, my car will never be slower than a rocket bike.
The rocket bike is a valid upper bound for my car's speed."

O = "Won't be worse than..." (Upper bound)

Ω = "Won't be better than..." (Lower bound)

Θ = "Grows exactly like..." (Tight bound - most informative!)

===========

Problem: 
is (x + y)^2 = O(x^2 + y^2)?

answer:
by the definitino of O(n), f(n) = O(g(n)) iff there exists constant c
such that  n f (n) <= c g(n)
(x+y)^2 <= (x^2 + y^2) for some constant c

LHS
(x + y) (x + y) 
= x^2 + 2xy + y^2 <= c(x^2 + y^2) for all sufficiently large x,y

we need to show 2xy is less than or equal to some constant times (x^2 + y^2)

since we know any real number squared is always >= 0
(x-y)^2 >= 0
(x^2 + 2xy + y^2) >= 0
x^2 + y^2 >= 2xy

x² + 2xy + y² ≤ x² + (x² + y²) + y²



 ===================

 Stop and think: transitive experience
 problem: show that big o relationship are transitive.
that is, if f (n) = O (g(n)) and
g(n) = O(h(n)), then f(n) = O(h(n))

answer:

f(n) = O(g(n))
by definition, f(n) = o(g(n)) iff there exists constant c such that 
n . f(n) <= c g(n)
this means f grows much slower than g

g(n) = O(h(n))
n . g(n) <= c h(n)
this means g grows slower than h

therefore if f < g, then g < h
so f < g < h

Stop and think:
importance of an even split

problem: how many queries does binary search take
on the million-name manhattan phonebook if each split was 1/3 to 2/3 instead of 1/2 to 1/2?

answer:
original binary search -> log base 2 of n
each iteration you divide by half until its found

if each split was 1/3 to 2/3
log base 3 of n -> divide by 1/3?
my guess is growth rate is tsill the same by log n since changing the base doesn't matter?

2.10
Exercises

Program analysis

2.1
What value is returned by the following function? Express your answer as a
function of n. Give the worst-case running time using the Big Oh notation.

function mystery(n)
r := 0
for i := 1 to n−1 do
    for j := i+1 to n do
        for k := 1 to j do
            r := r+1
return(r)

 a. what is the value returned?
answer: lets trace this with n = 3
lets try another input
mystery(3)
r = 0
outer loop: i = 1  to 2

when i = 1
    mid loop
    j = 2 to 3
    
    j = 2
        inner loop: k = 1 to j
        k = 1 to 2
        k = 1 -> r + 1 = 1
        k = 2 -> r + 1 = 2
    
    j = 3
        inner loop: k = 1 to j
        k = 1 to 3
        k = 1 -> r + 1 = 3
        k = 2 -> r + 1 = 4
        k = 3 -> r + 1 = 5

when i = 2
    mid loop
    j = 3 to 3

    j = 3
        inner loop: k = 1 to 3
        k = 1 -> r + 1 = 6
        k = 2 -> r + 1 = 7
        k = 3 -> r + 1 = 8

return 8

mystery(4)
r = 0
outer loop: i = 1  to 3

i = 1
    j = 2 to 4
    j = 2
        k = 1 to 2
        k = 1
            r = 0 + 1 = 1
        k = 2
            r = 1 + 1 = 2
    j = 3
        k = 1 to 3
        k = 1
            r = 2 + 1 = 3
        k = 2
            r = 3 + 1 = 4
        k = 3
            r = 4 + 1 = 5
    j = 4
        k = 1 to 4
        k = 1
            r = 5 + 1 = 6
        k = 2
            r = 6 + 1 = 7
        k = 3
            r = 7 + 1 = 8
        k = 4
            r = 8 + 1 = 9

i = 2
    j = 3 to 4
    j = 3
        k = 1 to 3
        k = 1
            r = 10
        k = 2
            r = 11
        k = 3
            r = 12
    j = 4
        k = 1 to 4
        k = 1
            r = 13
        k = 2
            r = 14
        k = 3
            r = 15
        k = 4
            r = 16

i = 3
    j = 4 to 4
    j = 4
        k = 1 to 4
        k = 1
            r = 17
        k = 2
            r = 18
        k = 3
            r = 19
        k = 4
            r = 20

return r = 20

n(3) return 8
n(4) returns 20
it seems to me the function acts like a simple countre
but designed by nested loops

 b. worst case running time using big O notation
 since the function has 3 loops,
 that's n * n * n + 1 operation = n^3+1
 so in terms of big O notation =  O(n^3)

 2.2 what value is returned by the following function?
 express your answer as a function of n
 give worst-case running time using big o notation

function pesky(n)
r = 0
for i := 1 to n do
    for j := 1 to i do
        for k := j to i+j do
            r := r+1
return(r)

answer:
lets trace pesky(1)
r = 0
outer -> i = 1 to 1

i = 1
    j = 1 to 1
    j = 1
        k = 1 to 2
        k = 1
            r = 0 + 1 = 1
        k = 2
            r = 1 + 1 = 2
return 2

pesky(2)
r = 0
ouer -> i = 1 to 2
i = 1
    j = 1 to 2
    j = 1
        k = 1 to 2
        k = 1
            r = 0 + 1 = 1
        k = 2
            r = 2
    j = 2
        k = 2 to 3
        k = 2
            r = 3
        k = 3
            r = 4

i = 2
    j = 1 to 2
    j = 1
        k = 1 to 3
        k = 1
            r = 5
        k = 2
            r = 6
        k = 3
            r = 7
    j = 2
        k = 2 to 4
        k = 2
            r = 8
        k = 3
            r = 9
        k = 4
            r = 10
return 10

pattern: n=1 return 2, n=2 return 10
the sum from i=1 to n + the sum of j=1 to i of i + j

worst case running time: feels limilar to toe previous triangular thing
n * n * n since its 3 loops??

2-3
 2.2 what value is returned by the following function?
 express your answer as a function of n
 give worst-case running time using big o notation

function prestiferous(n)
r:= 0
 for i := 1 to n do
    for j := 1 to i do
        for k := j to i+j do
            for l := 1 to i+j −k do
             r := r+1
return(r)
similar to the previous one

trace example
r = 1

outer loop
i = 1 to 1
i = 1
    j = 1 to 1
    j = 1
        k = 1 to 2
        k = 1
            l = 1 to 1
        k = 2
            l = 1 to 0

worse case = 4  loops = n *n * n * n
it only runs when i + j - k > 0
O(n^4)

2.4 
 2.2 what value is returned by the following function?
 express your answer as a function of n
 give worst-case running time using big o notation

function conondrum(n)
r := 0
    for i := 1 to n do
        for j := i+1to n do
            for k := i+j −1 to n do
                r := r+1
return(r)

inner loop k runs when i + j - 1 < n

2.5
suppose the following algorithm is used to evaluate polynomial
p(x) = anx^n + an-1x^n-1 + ... + a1x + a0

p:= a0 ; lowest term?
xpower := 1 ; curr power? initialize power to 1
for i:=1 to n do
    xpower := x * xpower
    p:= p + ai * xpower
end

a. how many multpilications are done in the worst case? how many additions?
b. how many multiplications are done on the average?
c. can you improve this algorithm?

answer:
a. worst case:
if there are many terms
the main loop:

runs from 1 to n
only do one mltiplication and one addition
that is, update xpower to x * xpower
and update p to p + ai * xpower
for each terms,
so for the whole algorithm

pre loop + (operations per iteration * n)

b. how many multplications are done on the average?
Since the algorithm does the exact same work regardless of the coefficient values, the average case is identical to the worst case.



c. horner;s rule
accumulate the sum of term multiplications inside a variable
and sum it each iteration

2.6
prove that the following algorithm for computing the maximum value in a narray A[1...n]
is correct


function max(A)
    m:=A[1] ;set current max  value to first item in array A
    for i := 2 to n do  ; loop from 2nd item to n (total array length)
        if A[i] > m then m := A[i] ; check if current item's value is greater than m, if so this is new max found so far
    return (m)


initialization:
before the start of the iteration, m: A[1]
set the current max value to the Array's first item
since we've only seen one element so far, m correctly holds the maximum of the subarray A[1..1]

maintenance:
at the start of each iteration, the loop checks for the next current item of the ARray
it checks if there is a item greater than temporary max item initialized

assume m holds the maximum of a[1...i-1]. After comparing with A[i] m becomes max(m,A[i]) which is the maximums of A[1..i]

terminateion
loop terminates when i = n + 1
 At this point, m holds the maximum of A[1..n], which is exactly what we return.

 Big Oh

 2.7
 true or false?
 a. is 2^n+1 = O(2^n)?
 b. is 2^2n = O(2^n)?

 answer:
a. true because 2^n+1 is just 2^n . 2^1 which is just O(2^n)
b. false because 2^2n <= 2^n . c

2.8
for each of the following pairs of funciton,
either f(n) is in O(g(n)),
f(n) is in Omega(g(n)),
or f(n) = theta(g(n)) determine correct relationship and explain why

a. f(n) = log n^2;
g(n) = log n + 5

fn dominant growth = log n^2 = 2 . log(n)
gn dominant growth = log n

since the grows are the same
lowre bound is the same and upper bound is the same therefore 
f(n) is in theta(g(n))

b. f(n) = sqrt(n)
g(n) = log n^2

fn growth = sqrt(n)
gn growth = log(n^2) = 2 . log(n)

since sqrt grows faster than log,

f(n) <= c . g(n)
but sqrt n grows faster than log n
f(n) = Omega(g(n))
g(n) = O(f(n))

c. f(n) = log^2 n
g(n) = log n

fn growth = log^2 n = log^2 = log n . log n
gn growth log n

n = 100 -> log n = 6.6, log^2 n = 43.6
n = 1000 -> log n = 20, log^2 n = 400
gn grows slower since its log n

therefore fn >= gn
so fn = big omega (gn)
gn = big O (fn)

d. f(n) = n
g(n) = log^2 n

here n grows linearly
n = 100 -> log n^2 = 44
n = 1000 -> log n^2 = 400

n grows faster than log^2 n
therefore

fn > gn
so f(n) = big omega(g(n))
and g(n) = big o (f(n))

e. f(n) = n log n + n
g(n) = log n

for n = 10
f(10) = 10 . log2 10 + 10 = 10 . 3.3 + 10 = 43
g(10) = log2 10 = 3.3

for n = 100
f(100) = 100 . log2 100 + 100 = 100 . 6.64 + 100 = 664 + 100 = 764
g(100) = log 100 = 6.64

therefore fn grows faster than gn
fn > gn
therefore fn = big omega (g(n))
and g(n) = big O (f(n))

f. f(n) = 10
g(n) = log 10 = 3.3

fn grows faster here
fn/gn = 10/3 = ~3
but the growth doens't pass each other its always linearly

fn = O(1)
gn = O(1)
so fn = big theta of gn
and gn = big theta of fn

g. f(n) = 2^n,
g(n) = 10n^2

n = 10
2^10 = 1024
10.10^2 = 1000

fn grows faster than gn - exponential n growth
so fn > gn
therfore fn = big omega (gn)
2^n = big omega(10n^2)

and gn = big O (fn)
10n^2 = big O (2^n)

h. f(n) = 2^n
g(n) = 3^n

n = 10
f(10) = 2^10 = 1024
g(10) = 3^10 = 59049
ratio = 0.01

n = 20
f(10) = 2^10 = 1048576
g(10) = 3^10 = 3486784401
ratio = 0.0003

therefore 3^n grows faster
gn > fn
so g(n) = big Omega (fn)
and f(n) = big O (gn)

Any cⁿ vs dⁿ where d > c → dⁿ dominates

2.9
for each following pairs of function fn and gn
determine wheter fn = O(g(n)) , gn = O(f(n)) or both

a. fn = (n^2-n)/2
gn = 6n

fn -> n^2 growth dominates
gn -> n growth

therefore fn grows faster than gn
so fn > gn, fn = big omega gn
and gn = O(f(n))

As n → ∞, does f(n)/g(n) approach a constant or infinity

b. fn = n + 2sqrt(n)
g(n) = n^2

fn -> n growth dominates
gn -> n^2 growth

therefore gn > fn
so gn = big omega (fn)
and fn = O(g(n))
n = O(n^2)

c. fn = n log n
gn = n sqrt n / 2

fn -> n log n growth
gn -> n . n^1/2 / 2 = n^1.5

 gn grows faster than fn
 so gn > fn
 therefore fn = O(g(n))

d. f(n) = n + log n
g(n) = sqrt(n)

which one grows faster?
n = 10
f(10) = 10 + log2 10 = 10 + 3.3
g(10) = sqrt 10 = 3.1
ratio fn/gn = ~1.06

n = 100
f(100) = 100 + log2 100 = 100 + 6.6 = 106.6
g(100) = sqrt 100 = 3.1 = 10
ratio = 10.6

therefore fn dominates and grows faster
so fn > gn, and gn = O(f(n))

e. 
fn = 2(log n)^2
gn = log n + 1


fn grows faster than gn since it grows like log n . log n
therfore fn > gn, so gn = O(f(n))

f. fn = 4n . log n + n
gn = (n^2-n) / 2

fn = n dominates
gn = n^2 dominates
therefore gn > fn
so fn = O(g(n))

2-10
prove that n^3 - 3n^2 - n + 1 = big theta (n^3)

to prove big theta we must show that
fn = big omega (gn)
and gn = big O (fn)

n^3 - 3n^2 - n + 1
here the term n^3 dominates

by the edfinition of big theta, f(n) = big theta (g(n))
c1 . g(n) <= f(n) <= c2 . g(n) for all n >= n0

c1 . n^3 <= n^3 - 3n^2 - n + 1 <= c2 . n^3

find the upper bound
we need to show  fn <= c2 . n^3
n^3 - 3n^2 - n + 1 <= n^3
c2 = 1

find the lower bound
we need to show  fn <= c2 . n^3
c1 . n^3 <= n^3 - 3n^2 - n + 1
factor out n^3 -> n³(1 - 3/n - 1/n² + 1/n³)
c1 = 1/2
1/2 - 3/n - 1/n² + 1/n³ ≥ 0
(1/2)n³ - 3n² - n + 1 ≥ 0
Upper bound: c₂ = 1, works for all n ≥ 1
Lower bound: c₁ = 1/2, works for all n ≥ 10
Therefore: n₀ = 10

2.11
Prove that n^2 = O(2^n)
answer:
by definition of big O, fn = O(g(n))
fn <= c2 . 2^n for all n >= 0

n = 4
fn = n^2 = 16
gn = 2^4 = 16
ratio fn/gn = 1

n = 5
fn = n^2 = 25
gn = 2^5 = 32
ratio fn/gn = ~0.7

crossover point 2^5 beats n^2
as n reaches infinity ratio fn/gn reaches 0

2.12
for each of the following pair of function fn and gn
give appropriate positive constant sch that fn <= c . gn for all n > 1
a. fn = n^2 + n + 1,
gn = 2n^3

fn -> grows like n^2
gn -> grow like n^3
n^3 grows faster than n^2, therefoer n^3 > n^2

fn <= c . gn for all n > 1
n^2 + n + 1, <= c . 2n^3

n=1 
fn = n^2 + n + 1 = 3
gn = 2n^3 = 2
here fn > gn
ratio = 3/2 = 1.5

n=2
fn = 2^2 + 2 + 1 = 7
gn = 2 . 2^3 = 16
ratio = ~0.4

constant that work = c = 1.5

b. fn = n . sqrt(n) + n^2,
gn = n^2

fn => n^2 growth dominates
gn => n^2 growh

we need  to find
fn <= c . gn for all n > 1

n = 1
fn = 1 . sqrt(1) + 1^2 = 3
gn = 1^2 = 1
ratio = 2

n = 2
fn = 2 . 1.4 + 2^2 = 2.8 + 4 = 4.8
gn = 2^2 = 4
ratio = ~1.2  

n = 3
fn = 3 . 1.7 + 3^2 = 5.19 + 9 = 14.19
gn = 3^2 = 9
ratio = ~1.567

n = 4
fn = 4 . 2 + 4^2 = 24
gn = 4^2 = 8
ratio = ~1.5

therefore amellst constant that satisfies is c = 2

at n = 1
f(1) = 2 <= 2 . 1 = 2

c. fn = n^2 - n + 1
gn = n^2 / 2

fn -> grows like n^2
gn -> grows like n^2


we need  to find
fn <= c . gn for all n > 1

n = 1
fn = 1^2 - 1 + 1 = 1-1+1 = 1
gn = n^2 / 2 = 1/2
ratio = 2

n = 2
fn = 2^2 - 2 + 1 = 4-2+1 = 3
gn = n^2 / 2 = 4/2 = 2
ratio = 1.5

n = 3
fn = 3^2 - 3 + 1 = 7
gn = 3^2 / 2 = 4.5
ratio = 1.556

largest ratio = 2 where n =1
need to find c such as the equation holds
f(n) = 1
c . g(1) = 2 . (1/2) = 1
1 <= 1
c = 2

2.13
prove that if f1(n) = O(g1(n)) and f2(n) = O(g2(n))
then f1(n) + f2(n) = O(g1(n)) + g2(n))

answer:
Since f₁(n) = O(g₁(n)), there exist constants c₁ > 0 and n₁ such that:
f₁(n) ≤ c₁·g₁(n) for all n ≥ n₁

Since f₂(n) = O(g₂(n)), there exist constants c₂ > 0 and n₂ such that:
f₂(n) ≤ c₂·g₂(n) for all n ≥ n₂
then
f1(n) + f2(n) = c1 . g1(n) + c2 . g2(n)

2.14
Prove that if f1(N)=Ω(g1(n)) and f2(n)=Ω(g2(n)), then f1(n)+f2(n)=
 Ω(g1(n)+g2(n)).

 answer:
 since f1n = Ωg1(n) and f2n = Ωg2(n),
  then f1n + f2n = Ω(g1n + g2n)

the sum of two functions is govern by the dominant one
Ω(gn + gn) -> Ω(max gn, gn)

since f1n = Ωg1n, there exist constant c1 > 0 and n1 such that
f1n >= c1 . g1n for all n >= n1

since f2n = Ωg2n, there exist constant c2 > 0 and n2 such that
f2n >= c2 . g2n for all n >= n2

then
f1n + f2n >= c1 . g2n + c2 . g2n

2.15
3] Prove that if f1(n)=O(g1(n)) and f2(n)=O(g2(n)), then f1(n) · f2(n)=
 O(g1(n) · g2(n))

 since f1n = og1n and f2n = og2n
 then f1n . f2n = o (g1n . g2n)

 by definition of big o,
 there cists some constant c1 and n1 such that
 f1n <= c1 . g1n for all n >= n1

 and 
 f2n <= c2 . g2n for all n >= n2

if f1 <= c1 . g1 and f2 <= c2 . g2 then
 f1 . f2 <= c1 . g1 . c2 . g2
f₁·f₂ ≤ (c₁·g₁)·(c₂·g₂) = (c₁·c₂)·(g₁·g₂)
f₁(n)·f₂(n) ≤ C·[g₁(n)·g₂(n)] for all n ≥ N

let C = c1 .c2 and  N = max(n1, n2)
then for all n >= N, we have n >= n1 and n >= n2
therefore equalitties hold

2.16
prove for all k>=1 and all sets of constants {ak, ak-1, ..., a1, a0} E R,
akn^k + ak-1n^k-1 + ... + a1n + a0 = O(n^k)

answer:
by definition of big O, there exists constant c and n0 such that
fn <= c . gn for all n >= n0
akn^k + ak-1n^k-1 + ... + a1n + a0 <= c . n^k
= ...

2.17 
[5] Show that for any real constants a and b, b>0
 (n +a)^b =Θ(n^b)

by definition of big theta there exist ocnstant c1 and c2 and n1 and n2 such that

fn = theta(gn)
fn <= c1. gn and fn >= c2 . gn

part 1 - upper bound
(n+a)^b <= c1 . n^b
???


2.18
List the functions below from the lowest to the highest order. If any two or
 more are of the same order, indicate which.

 n
 2n
 ln n
 n−n^3+7n^5
 lg n
 sqrt n
 e^n
n^2 + lg n
n^2
2^n-1
lg lg n
n^3
(lg n)^2
n!
n^1+e where 0 < e < 1

answer
lowest to highest growth
5. lg lg n
1. lg n
2. ln n
4. (lg n)^2
6. sqrt n
7. n
8. n^1+e
9. n^2
10. n^2 + lg n
11. n^3
12. n - n^3 + 7n^5
13. 2^n-1
14. 2^n
15. e^n
16. n!

2.19 list thefunctions below from lowest to highest order
if any two or more are the same indicate which

sqrt n
n
2^n
n log n
n - n^3 + 7n^5
n^2 + log n
n^2
n^3
log n
n^1/3 + log n
(log n)^2
n!
ln n
n/log n
log log n
(1/3)^n
(3/2)^n
6

answer:
lowest to hiwghest growth
1/3^n
6
log log n
log n
ln n
(log n)^2
n^1/3 + log n
sqrt n
n / log n
n
n log n
n^2
n^2 + log n
n^3
n - n^3 + 7n^5
3/2^n
n^2
2^n
n!

2.20
find two functions fn and gn that satisfy the following relationship
if no such f and g exist write none
a.  f(n)=o(g(n)) and f(n) does not equal Θ(g(n))
b.f(n)=Θ(g(n)) and f(n)=o(g(n))
c.  f(n)=Θ(g(n)) and f(n) does not equal  O(g(n))
d. f(n)=Ω(g(n)) and f(n) does not equal O(g(n))

answer:


2.21
true or false?
a. 2n^2 + 1 = O(n^2)
answer:

by definition of big O notation, there exists constant c and n such that
fn <= O(gn) 

fn -> grows like n^2
gn -> n^2 growth

n=1
fn = 2(1)^2 + 1 = 5
gn = 1^2 = 1
ratio = 5

n = 100
fn = 2(100)^2 + 1 = 20001
gn = 100^2 = 10000
ratio = ~2

n = 1000
fn = 2(1000)^2 + 1 = 2000001
gn = 1000^2 = 1000000
ratio = ~2

ratio fn/gn = 2n^2 + 1 / n^2
= 2 + 1/n^2

as n reaches infinity, it approaches a constant ~2
it is bounded above by some constant multiple of n^2
since the dominant growth is the same (n^2) therefore true


b. sqrt n = O(log n)
by definition of big O notation, there exists constant c and n such that
fn <= O(gn) for all sufficiently large n
or sqrt n <= log n

fn -> grows like sqrt n
gn -> log n growth - slower than sqrt n
is fn <= gn? since its slowe it should be fn >= gn or fn = big omega gn

n = 1
fn = sqrt 1 = 1
gn = log n = 0
ratio = 1

n = 100
fn = 10
gn = ~6.6
ratio = ~1.5

n = 10000
fn = 100
gn = 13
ratio = ~7.6

ratio of fn / gn = sqrt n / log n
as n approaches infinity, the ratio reaches infinity
therefore false


c. log n = O(sqrt n)
this is the reverse of the previous question

fn -> log n growth, slow
sqrt n -> sqrt n grwoth
so gn should be faster than fn -> fn <= gn

n = 1
fn = log n = 0
gn = sqrt 1 = 1
ratio = 0

n = 100
fn = ~6.6
gn = 10
ratio = ~0.66

n = 10000
fn = 13
gn = 100
ratio = ~0.13

ratio of fn / gn = sqrt n / log n
as n approaches infinity, the ratio reaches infinity
therefore there exist some constant c such that log n <= c . sqrt n for all sufficiently large n
therefore its true, fn <= gn

d. n^2(1 + sqrt n) = O (n^2 log n)
e. 3n^2 + sqrt n = O(n^2)
f. sqrt n log n = O(n)
g. log n = O(n^-1/2)


2.22
