
Theta notation
f(n) = the original function
g(n) = the function we're ocmparing with

Theta of g(n) means
the set of all functions that grow the same rate as g(n)

f(n) = theta of g(n) means
f(n) grows proportionlally and they only differ in constatn factors

f(n) = 2n^2 + 500n
g(n) = n^2

f(n) = theta of g(n)

===================================

Big O notation

theta of g(n) is a subset of big O of g(n)

O(g(n))
no matter how hard things get, f(n) will never gorw faster than a constant times g(n)
once n is big enough

worst case

===================================

Omega notation

no slower than g(n)
or the lower bound


====================

2n^2 + 3n + 1 = 2n^2 + theta of  (n)
all those messy constatnt that grows no longer than n^2, just ignore it



Exercises
3.1-1
Let f(n) and g(n) be asymtotically nonnegative functions.
using the basic definition of big theta notation, prove that

max(f(n),g(n)) = big theta of (f(n) + g(n))

answer:
we know that big theta means its sandwitched between the two
so its c1 of g(n) <= f(n) <= c2 of g(n) or something like that so it has upper end and lowerend
and the two functions grow at the same rate up to constants

so we need to find c1(f(n)+g(n)) <= max(f(n),g(n)) <= c2(f(n)+g(n))

and max function search for the maximum value
and we know sum  is at least always as big as the bigger one
and the max sum ratio is no larger than factor of 2

so max(f(n),g(n)) <= f(n) + g(n) <= 2.max(f(n),g(n))

3.1-2
show that for any real constants of a and b, where b>0
(n+a)^b = big theta of (n^b)

rewrite n+a as n(1+a/n)
(n+a)^b = [n(1+a/n)]^b

(n+a)^b = n^b(1+a/n)^b

constants L and U
L <= n^b(1 + a/n)^b <= U
what simple constants holds up?
L = (1/2)^b
U = (3/2)^b

3.1-3
Explain why the statmenet "the running time of algorithm A is at least O(n^2)"
is meaningless

answer: because big o notation measures worst case? and to calculate the least
maybe use big omega?
Upper bound: “The running time is at most proportional to n²” → use O(n²).

Lower bound: “The running time is at least proportional to n²” → use Ω(n²).

Tight bound: “The running time grows like n²” → use Θ(n²).

3.1-4

is 2^n+1 = O(2^n)?
is 2^2n = O(2^n)?


does there exist constant c > 0 such that f(n) <= c . g(n) for all large n?

divide both sides by g(n)
2^n+1 / 2^n = 2(n+1-n) = 2^1 = 2
true because its a constant

2^2n / 2^n = 2^(2n-n) = 2^n = 2n
false, this grows without bound

3.1-5
Prove theorem 3.1
Theorem 3.1
 For any two functions f.n/ and g.n/,wehavef.n/ D ‚.g.n// if and only if
 f.n/ D O.g.n// and f.n/ D .g.n//.

 if f(n) = theta(g(n)) , then show f(n) = O(g(n)) and f(n) = omega(g(n))
and prove the other way around

the constant c1, c2 amd n0 exist when f(n) = theta(g(n))

c1 g(n) <= f(n) <= c2 g(n)

since f(n) is between c1 g(n) and c2 g(n) then its
<= some constant times g(n) => thats the uppper bound, big o
>= some constant times g(n) => thats the lower bound, big omega

now prove the other way around

if  f(n) = O(g(n)) and f(n) = omega(g(n)), show f(n) = theta(g(n))

we know from definition big O is the upper bound <= some constant times g(n)
big omega some constant times g(n)
combine it

c1 (g(n)) <= f(n)
c2 (g(n)) >= f(n)
therefore  its sandwitched c1 g(n) <= f(n) <= c2 g(n)

f <= g --- upper bound O
f >= g --- lower bound Omega


3.1-6
proev that the running time of an algorithm is theta g(n) if and only if 
its worst case running time is O(g(n))
and best case is big omega(g(n))

answer:

by the definition of big theta notation we konw there exist some constatnt
c1 g(n) <= f(n) <= c2 g(n)

we know from definition big o notation is the upper bound f <= g
and omega lower bound f >= g
and by definition of theta = O + omega

Tbest(n) <= T(n) <= Tworst(n)

3.1-7
prove that o(g(n)) intersect w(g(n)) is he empty set